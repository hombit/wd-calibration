{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-13T16:59:49.957406Z",
     "start_time": "2023-07-13T16:59:46.879694Z"
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchbnn as bnn\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "class BNN(nn.Module):\n",
    "    def __init__(self, in_features: int):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=in_features, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=256, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=64, out_features=32),\n",
    "            nn.ReLU(),\n",
    "            bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=32, out_features=1),\n",
    "        )\n",
    "\n",
    "    def to(self, device, *args, **kwargs):\n",
    "        super().to(device, *args, **kwargs)\n",
    "        for layer in self.layers:\n",
    "            layer.to(device, *args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class BNNTask(pl.LightningModule):\n",
    "    def __init__(self, model: BNN, learning_rate=3e-4, kl_weight=0.1):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.kl_weight = kl_weight\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.kl_loss = bnn.BKLLoss(reduction=\"mean\", last_layer_only=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def _loss(self, batch):\n",
    "        x, y = batch\n",
    "        predictions = self.model(x).squeeze()\n",
    "        mse = self.mse_loss(predictions, y.squeeze())\n",
    "        kl = self.kl_loss(self.model)\n",
    "        return mse + self.kl_weight * kl\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._loss(batch)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._loss(batch)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "\n",
    "class GaussianModel(nn.Module):\n",
    "    def __init__(self, in_features: int, device=None, min_var: float = 1e-9):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.min_var = min_var\n",
    "        self.device = device\n",
    "\n",
    "        self.hidden_layers = nn.Sequential(\n",
    "            # nn.Linear(self.in_features, 256),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Dropout(0.5),\n",
    "            # nn.Linear(256, 64),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(64, 32),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(32, 1),\n",
    "        )\n",
    "        # last layer\n",
    "        self.output_layer = nn.Linear(self.in_features, 1, bias=True)\n",
    "\n",
    "        cov_matrix_ndim = self.output_layer.in_features + 1  # +1 is for the bias\n",
    "        self.output_cov_matrix_root = nn.Parameter(torch.randn(cov_matrix_ndim, cov_matrix_ndim, requires_grad=True, device=self.device))\n",
    "\n",
    "    def to(self, device, *args, **kwargs):\n",
    "        super().to(device, *args, **kwargs)\n",
    "        self.device = device\n",
    "        self.output_cov_matrix_root = self.output_cov_matrix_root.to(device, *args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def features(self, x):\n",
    "        return self.hidden_layers(x)\n",
    "\n",
    "    def output_cov_matrix(self):\n",
    "        cov_matrix = self.output_cov_matrix_root.T @ self.output_cov_matrix_root\n",
    "        cov_matrix[np.diag_indices(cov_matrix.shape[0])] += self.min_var\n",
    "        return cov_matrix\n",
    "\n",
    "    def variance(self, features):\n",
    "        # ones are for the bias\n",
    "        features_and_ones = torch.hstack([features, torch.ones((features.shape[0], 1), device=features.device)])\n",
    "        k = features_and_ones.shape[1]\n",
    "        return features_and_ones.reshape(-1, 1, k) @ self.output_cov_matrix()[None, :, :] @ features_and_ones.reshape(-1, k, 1)\n",
    "\n",
    "    def forward(self, x, return_var=True):\n",
    "        features = self.features(x)\n",
    "        output = self.output_layer(features)\n",
    "        if return_var:\n",
    "            return output, self.variance(features)\n",
    "        return output\n",
    "\n",
    "\n",
    "class GaussianTask(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate=3e-4):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x, y_err):\n",
    "        return self.model(x, y_err)\n",
    "\n",
    "    def _loss(self, batch):\n",
    "        x, y, y_err = batch\n",
    "        # predictions, variance = self.model(x, return_var=True)\n",
    "        # predictions, variance = predictions.squeeze(), variance.squeeze()\n",
    "        # variance += torch.square(y_err)\n",
    "        # return torch.mean(torch.square(predictions - y.squeeze()) / variance + torch.log(variance))\n",
    "        predictions = self.model(x, return_var=False).squeeze()\n",
    "        return torch.mean(torch.square(predictions - y.squeeze()))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._loss(batch)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._loss(batch)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "\n",
    "def train_nn(X, y, y_err, *, batch_size: int = 1 << 10, n_epoch: int = 10_000, learning_rate: float = 3e-4):\n",
    "    x_train, x_val, y_train, y_val, y_err_train, y_err_val = (\n",
    "        torch.tensor(a, dtype=torch.float32)\n",
    "        for a in train_test_split(X, y, y_err, test_size=0.4, random_state=42)\n",
    "    )\n",
    "\n",
    "    model = GaussianModel(x_train.shape[1])\n",
    "    task = GaussianTask(model, learning_rate=learning_rate)\n",
    "\n",
    "    # model = BNN(x_train.shape[1])\n",
    "    # task = BNNTask(model, learning_rate=learning_rate, kl_weight=0.1)\n",
    "\n",
    "    train_dataset = TensorDataset(x_train, y_train, y_err_train)\n",
    "    val_dataset = TensorDataset(x_val, y_val, y_err_val)\n",
    "    # train_dataset = TensorDataset(x_train, y_train)\n",
    "    # val_dataset = TensorDataset(x_val, y_val)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=1 << 14)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=n_epoch,\n",
    "        # accelerator='auto',\n",
    "        accelerator='cpu',\n",
    "        enable_progress_bar=False,\n",
    "        logger=True,\n",
    "        callbacks=[\n",
    "            EarlyStopping('val_loss', patience=10, mode='min'),\n",
    "        ]\n",
    "    )\n",
    "    trainer.fit(task, train_dataloader, val_dataloader)\n",
    "\n",
    "    return model, task, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def linear_fit(X, y, y_err, model, jac):\n",
    "    sqrt_w = 1 / y_err\n",
    "    A = sqrt_w[:, None] * jac(X)\n",
    "    params, (chi2,), *_ = np.linalg.lstsq(A, sqrt_w * y, rcond=None)\n",
    "    reduced_chi2 = chi2 / (A.shape[0] - A.shape[1])\n",
    "    covariance = np.linalg.inv(A.T @ A) * reduced_chi2\n",
    "\n",
    "    # print(f'{params = }')\n",
    "    # print(f'{covariance = }')\n",
    "    # print(f'{reduced_chi2 = }')\n",
    "\n",
    "    def f(x, cov=True):\n",
    "        x = np.atleast_2d(x)\n",
    "        y = model(x, params)\n",
    "\n",
    "        if not cov:\n",
    "            return y\n",
    "\n",
    "        j = jac(x)\n",
    "        var = j @ covariance @ j.T\n",
    "        return y, var\n",
    "\n",
    "    return f, params, covariance\n",
    "\n",
    "\n",
    "def non_linear_fit(X, y, y_err, model):\n",
    "    from scipy.optimize import curve_fit\n",
    "\n",
    "    result = curve_fit(\n",
    "        f=lambda x, *params: model(x, np.array(list(params))),\n",
    "        xdata=X,\n",
    "        ydata=y,\n",
    "        sigma=y_err,\n",
    "        p0=np.zeros(X.shape[1] + 1),\n",
    "    )\n",
    "    print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-13T17:19:43.119123Z",
     "start_time": "2023-07-13T17:19:43.108899Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "n_features = 3\n",
    "n_samples = 1 << 20\n",
    "\n",
    "def model(x, params):\n",
    "    y = x @ params[:-1, None] + params[-1]\n",
    "    return y.squeeze()\n",
    "\n",
    "\n",
    "def jac(x):\n",
    "    return np.hstack([x, np.ones((x.shape[0], 1))])\n",
    "\n",
    "\n",
    "def true_noise_amplitude(x):\n",
    "    # return np.full(x.shape[0], 0.5)\n",
    "    return 0.05 + 0.5 * np.linalg.norm(x, axis=1)\n",
    "\n",
    "\n",
    "def generate_data(params, n_samples: int = 1 << 20, rng=0):\n",
    "    rng = np.random.default_rng(rng)\n",
    "\n",
    "    X = rng.uniform(-1, 1, size=(n_samples, n_features))\n",
    "    latent = rng.uniform(0, 2, size=(n_samples, 1))\n",
    "    y_err = true_noise_amplitude(X)\n",
    "    y_known_noise = rng.normal(loc=0, scale=y_err)\n",
    "    y_unknown_noise = 0 * rng.normal(loc=0, scale=true_noise_amplitude(latent))\n",
    "    y = model(X, params) + y_known_noise + y_unknown_noise\n",
    "\n",
    "    return X, y, y_err\n",
    "\n",
    "\n",
    "true_params = np.array([+2.0, -3.0, +1.0, +0.5])\n",
    "assert true_params.size == n_features + 1\n",
    "\n",
    "X, y, y_err = generate_data(true_params, n_samples=1 << 20)\n",
    "\n",
    "X_train, X_test, y_train, y_test, y_err_train, y_err_test = train_test_split(X, y, y_err, test_size=0.2, random_state=42, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-13T17:29:25.663847Z",
     "start_time": "2023-07-13T17:29:25.515526Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30094158973569707\n",
      "(array([ 0.78355059, -1.1703901 ,  4.44560298]),\n",
      " array([[ 1.66216401e-06,  8.64392630e-08, -5.97198433e-07],\n",
      "       [ 8.64392630e-08,  1.69493379e-06,  2.17300407e-07],\n",
      "       [-5.97198433e-07,  2.17300407e-07,  2.21819043e-06]]))\n"
     ]
    }
   ],
   "source": [
    "linear_f, params, cov = linear_fit(X_train, y_train, y_err_train, model, jac)\n",
    "# non_linear_fit(X_train, y_train, y_err_train, model)\n",
    "print(np.mean(np.square(linear_f(X_test, cov=False) - y_test)))\n",
    "pprint(linear_f(X_test[:3], cov=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-13T17:29:26.582362Z",
     "start_time": "2023-07-13T17:29:26.496742Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:08<00:00, 1161.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.00221801 -2.99992089  1.00080127  0.50053013]\n",
      "[[ 1.12496224e-04 -1.73328443e-06 -2.02285541e-06  3.79410850e-07]\n",
      " [-1.73328443e-06  1.10778150e-04  2.30503376e-06  2.69186347e-07]\n",
      " [-2.02285541e-06  2.30503376e-06  1.08547284e-04  6.26235834e-07]\n",
      " [ 3.79410850e-07  2.69186347e-07  6.26235834e-07  2.36066733e-05]]\n",
      "[[ 1.63068611e-04  8.37864034e-06 -5.79936991e-05]\n",
      " [ 8.37864034e-06  1.73394440e-04  1.85234150e-05]\n",
      " [-5.79936991e-05  1.85234150e-05  2.18637810e-04]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "params_bootstrap = []\n",
    "linear_f_bootstrap = []\n",
    "from tqdm import tqdm\n",
    "for _i_bootstrap in tqdm(range(10000)):\n",
    "    idx = rng.choice(X_train.shape[0], size=X_train.shape[0] // 100, replace=True)\n",
    "    linear_f_bootstrap_iter, params_bootstrap_iter, _cov = linear_fit(X_train[idx], y_train[idx], y_err_train[idx], model, jac)\n",
    "    linear_f_bootstrap.append(linear_f_bootstrap_iter)\n",
    "    params_bootstrap.append(params_bootstrap_iter)\n",
    "params_bootstrap_mean = np.mean(params_bootstrap, axis=0)\n",
    "params_bootstrap_cov = np.cov(np.array(params_bootstrap).T)\n",
    "print(params_bootstrap_mean)\n",
    "print(params_bootstrap_cov)\n",
    "print(np.cov([linear_f_bootstrap[i](X_test[:3], cov=False) for i in range(len(linear_f_bootstrap))], rowvar=False))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-13T18:03:30.121433Z",
     "start_time": "2023-07-13T18:03:21.482833Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/hombit/.virtualenvs/calibration/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name  | Type          | Params\n",
      "----------------------------------------\n",
      "0 | model | GaussianModel | 20    \n",
      "----------------------------------------\n",
      "20        Trainable params\n",
      "0         Non-trainable params\n",
      "20        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "/Users/hombit/.virtualenvs/calibration/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/hombit/.virtualenvs/calibration/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "nn_model, nn_task, nn_trainer = train_nn(X_train, y_train, y_err_train, n_epoch=50, learning_rate=1e-3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-03T21:17:22.254210Z",
     "start_time": "2023-07-03T21:14:34.177987Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0.7715461, -1.1633983,  4.4548364], dtype=float32)"
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model(torch.tensor(X_test[:3], dtype=torch.float32), return_var=False).detach().numpy().squeeze()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-03T21:23:16.478572Z",
     "start_time": "2023-07-03T21:23:16.475217Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nn_task._loss((torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32), torch.tensor(y_err_test, dtype=torch.float32)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mu, var = nn_model(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_err_test, dtype=torch.float32)).T\n",
    "# torch.mean(torch.square(torch.tensor(y_test, dtype=torch.float32) - mu) / (var + torch.tensor(y_err_test, dtype=torch.float32)**2))\n",
    "torch.mean(torch.square(torch.tensor(y_test, dtype=torch.float32) - mu))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nn_model(torch.zeros((1, n_features)), torch.full((1,), 0.5))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
